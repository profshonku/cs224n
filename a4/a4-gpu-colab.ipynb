{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0wRWAuyYA5uch4irdm6SN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsoum/cs224n/blob/main/a4/a4-gpu-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P03JYoMT0bG5",
        "outputId": "0707f276-7afa-4c8c-bce4-19eaff696faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs224n'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 259 (delta 67), reused 150 (delta 61), pack-reused 100\u001b[K\n",
            "Receiving objects: 100% (259/259), 74.13 MiB | 11.16 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dsoum/cs224n.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cs224n/a4\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BFwho_r0wgG",
        "outputId": "a05685ba-957f-47f9-eaba-c0460a3aa258"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cs224n/a4\n",
            "/content/cs224n/a4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docopt sentencepiece"
      ],
      "metadata": {
        "id": "VV9pm7_p1Yp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sh run.sh vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GI-TUsF1NtI",
        "outputId": "9ec65eb8-40aa-4f54-8470-be1321324eb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "read in source sentences: ./chr_en_data/train.chr\n",
            "read in target sentences: ./chr_en_data/train.en\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: ./chr_en_data/train.chr\n",
            "  input_format: \n",
            "  model_prefix: src\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 21000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./chr_en_data/train.chr\n",
            "trainer_interface.cc(406) LOG(INFO) Loaded all 16696 sentences\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(536) LOG(INFO) all chars count=1024249\n",
            "trainer_interface.cc(547) LOG(INFO) Done: 99.95% characters are covered.\n",
            "trainer_interface.cc(557) LOG(INFO) Alphabet size=115\n",
            "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.9995\n",
            "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 16696 sentences.\n",
            "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
            "unigram_model_trainer.cc(201) LOG(INFO) Initialized 71584 seed sentencepieces\n",
            "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 16696\n",
            "trainer_interface.cc(607) LOG(INFO) Done! 54339\n",
            "unigram_model_trainer.cc(491) LOG(INFO) Using 54339 sentences for EM training\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=31853 obj=13.2384 num_tokens=118297 num_tokens/piece=3.71384\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=28580 obj=11.3757 num_tokens=118524 num_tokens/piece=4.1471\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=23086 obj=11.397 num_tokens=122695 num_tokens/piece=5.31469\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=22993 obj=11.3487 num_tokens=122755 num_tokens/piece=5.3388\n",
            "trainer_interface.cc(685) LOG(INFO) Saving model: src.model\n",
            "trainer_interface.cc(697) LOG(INFO) Saving vocabs: src.vocab\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: ./chr_en_data/train.en\n",
            "  input_format: \n",
            "  model_prefix: tgt\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 8000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./chr_en_data/train.en\n",
            "trainer_interface.cc(406) LOG(INFO) Loaded all 16696 sentences\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(536) LOG(INFO) all chars count=1642445\n",
            "trainer_interface.cc(547) LOG(INFO) Done: 99.9521% characters are covered.\n",
            "trainer_interface.cc(557) LOG(INFO) Alphabet size=61\n",
            "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999521\n",
            "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 16696 sentences.\n",
            "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
            "unigram_model_trainer.cc(201) LOG(INFO) Initialized 35820 seed sentencepieces\n",
            "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 16696\n",
            "trainer_interface.cc(607) LOG(INFO) Done! 24681\n",
            "unigram_model_trainer.cc(491) LOG(INFO) Using 24681 sentences for EM training\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=12950 obj=9.92202 num_tokens=48774 num_tokens/piece=3.76633\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=10769 obj=7.89767 num_tokens=48939 num_tokens/piece=4.54443\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=8796 obj=7.84656 num_tokens=50602 num_tokens/piece=5.75284\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=8788 obj=7.82771 num_tokens=50633 num_tokens/piece=5.76161\n",
            "trainer_interface.cc(685) LOG(INFO) Saving model: tgt.model\n",
            "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tgt.vocab\n",
            "initialize source vocabulary ..\n",
            "initialize target vocabulary ..\n",
            "generated vocabulary, source 21000 words, target 8000 words\n",
            "vocabulary saved to vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "EIinbSu72dOH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sh run.sh train_local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "vSn5k_PX1U-D",
        "outputId": "fc72f890-3d99-4d0b-b7b6-c4bce851e7b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2023-03-16 14:37:08.132484: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-16 14:37:09.000674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-16 14:37:09.000801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-16 14:37:09.000822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "uniformly initialize parameters [-0.100000, +0.100000]\n",
            "use device: cpu\n",
            "begin Maximum Likelihood training\n",
            "epoch 1, iter 10, avg. loss 198.23, avg. ppl 3086.08 cum. examples 320, speed 60.93 words/sec, time elapsed 129.58 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/cs224n/a4/run.py\", line 366, in <module>\n",
            "    main()\n",
            "  File \"/content/cs224n/a4/run.py\", line 358, in main\n",
            "    train(args)\n",
            "  File \"/content/cs224n/a4/run.py\", line 187, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fe29ef3c4837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh run.sh train_local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh run.sh train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s0_VQkN2Z_l",
        "outputId": "5ecb069b-c713-4ff3-e9af-6a07758a5112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation: iter 800, dev. ppl 57.825054\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 810, avg. loss 95.17, avg. ppl 46.17 cum. examples 320, speed 1031.54 words/sec, time elapsed 217.44 sec\n",
            "epoch 2, iter 820, avg. loss 86.79, avg. ppl 43.59 cum. examples 640, speed 3048.56 words/sec, time elapsed 219.85 sec\n",
            "epoch 2, iter 830, avg. loss 95.46, avg. ppl 45.05 cum. examples 960, speed 3471.75 words/sec, time elapsed 222.16 sec\n",
            "epoch 2, iter 840, avg. loss 87.99, avg. ppl 43.91 cum. examples 1280, speed 3152.70 words/sec, time elapsed 224.52 sec\n",
            "epoch 2, iter 850, avg. loss 93.17, avg. ppl 38.57 cum. examples 1600, speed 3320.82 words/sec, time elapsed 226.98 sec\n",
            "epoch 2, iter 860, avg. loss 93.83, avg. ppl 45.61 cum. examples 1920, speed 3253.94 words/sec, time elapsed 229.40 sec\n",
            "epoch 2, iter 870, avg. loss 92.91, avg. ppl 42.55 cum. examples 2240, speed 3233.99 words/sec, time elapsed 231.85 sec\n",
            "epoch 2, iter 880, avg. loss 89.79, avg. ppl 41.66 cum. examples 2560, speed 3031.95 words/sec, time elapsed 234.39 sec\n",
            "epoch 2, iter 890, avg. loss 86.93, avg. ppl 42.05 cum. examples 2880, speed 3363.10 words/sec, time elapsed 236.60 sec\n",
            "epoch 2, iter 900, avg. loss 90.30, avg. ppl 40.68 cum. examples 3200, speed 3533.59 words/sec, time elapsed 238.81 sec\n",
            "epoch 2, iter 910, avg. loss 90.09, avg. ppl 41.18 cum. examples 3520, speed 3155.24 words/sec, time elapsed 241.27 sec\n",
            "epoch 2, iter 920, avg. loss 89.37, avg. ppl 38.03 cum. examples 3840, speed 3334.14 words/sec, time elapsed 243.62 sec\n",
            "epoch 2, iter 930, avg. loss 92.34, avg. ppl 37.78 cum. examples 4160, speed 3331.78 words/sec, time elapsed 246.07 sec\n",
            "epoch 2, iter 940, avg. loss 85.70, avg. ppl 37.72 cum. examples 4480, speed 3157.73 words/sec, time elapsed 248.46 sec\n",
            "epoch 2, iter 950, avg. loss 89.69, avg. ppl 41.80 cum. examples 4800, speed 3136.40 words/sec, time elapsed 250.91 sec\n",
            "epoch 2, iter 960, avg. loss 85.65, avg. ppl 39.07 cum. examples 5120, speed 3036.65 words/sec, time elapsed 253.37 sec\n",
            "epoch 2, iter 970, avg. loss 84.30, avg. ppl 36.00 cum. examples 5440, speed 3480.88 words/sec, time elapsed 255.54 sec\n",
            "epoch 2, iter 980, avg. loss 97.14, avg. ppl 44.96 cum. examples 5760, speed 3175.49 words/sec, time elapsed 258.11 sec\n",
            "epoch 2, iter 990, avg. loss 90.25, avg. ppl 36.16 cum. examples 6080, speed 3358.86 words/sec, time elapsed 260.50 sec\n",
            "epoch 2, iter 1000, avg. loss 89.98, avg. ppl 39.73 cum. examples 6400, speed 3249.04 words/sec, time elapsed 262.91 sec\n",
            "epoch 2, iter 1000, cum. loss 90.34, cum. ppl 41.00 cum. examples 6400\n",
            "begin validation ...\n",
            "validation: iter 1000, dev. ppl 49.325574\n",
            "save currently the best model to [model.bin]\n",
            "save model parameters to [model.bin]\n",
            "epoch 2, iter 1010, avg. loss 89.86, avg. ppl 37.49 cum. examples 320, speed 1051.86 words/sec, time elapsed 270.46 sec\n",
            "epoch 2, iter 1020, avg. loss 84.85, avg. ppl 33.59 cum. examples 640, speed 3449.81 words/sec, time elapsed 272.70 sec\n",
            "epoch 2, iter 1030, avg. loss 86.64, avg. ppl 34.08 cum. examples 960, speed 3307.36 words/sec, time elapsed 275.07 sec\n",
            "epoch 2, iter 1040, avg. loss 87.14, avg. ppl 34.57 cum. examples 1280, speed 3292.33 words/sec, time elapsed 277.46 sec\n",
            "epoch 3, iter 1050, avg. loss 82.09, avg. ppl 30.66 cum. examples 1592, speed 2877.77 words/sec, time elapsed 280.06 sec\n",
            "epoch 3, iter 1060, avg. loss 74.52, avg. ppl 22.60 cum. examples 1912, speed 3242.78 words/sec, time elapsed 282.42 sec\n",
            "epoch 3, iter 1070, avg. loss 74.67, avg. ppl 24.57 cum. examples 2232, speed 3007.71 words/sec, time elapsed 284.90 sec\n",
            "epoch 3, iter 1080, avg. loss 75.05, avg. ppl 22.22 cum. examples 2552, speed 3437.84 words/sec, time elapsed 287.15 sec\n",
            "epoch 3, iter 1090, avg. loss 78.24, avg. ppl 22.94 cum. examples 2872, speed 3173.52 words/sec, time elapsed 289.67 sec\n",
            "epoch 3, iter 1100, avg. loss 77.41, avg. ppl 23.76 cum. examples 3192, speed 3143.74 words/sec, time elapsed 292.16 sec\n",
            "epoch 3, iter 1110, avg. loss 75.64, avg. ppl 22.76 cum. examples 3512, speed 3094.42 words/sec, time elapsed 294.66 sec\n",
            "epoch 3, iter 1120, avg. loss 76.15, avg. ppl 22.38 cum. examples 3832, speed 3242.91 words/sec, time elapsed 297.08 sec\n",
            "epoch 3, iter 1130, avg. loss 79.76, avg. ppl 24.19 cum. examples 4152, speed 2937.50 words/sec, time elapsed 299.81 sec\n",
            "epoch 3, iter 1140, avg. loss 80.33, avg. ppl 24.06 cum. examples 4472, speed 3196.86 words/sec, time elapsed 302.34 sec\n",
            "epoch 3, iter 1150, avg. loss 68.12, avg. ppl 19.65 cum. examples 4792, speed 3251.62 words/sec, time elapsed 304.59 sec\n",
            "epoch 3, iter 1160, avg. loss 79.38, avg. ppl 22.96 cum. examples 5112, speed 3245.00 words/sec, time elapsed 307.09 sec\n",
            "epoch 3, iter 1170, avg. loss 76.18, avg. ppl 22.65 cum. examples 5432, speed 3320.38 words/sec, time elapsed 309.44 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh run.sh test"
      ],
      "metadata": {
        "id": "A4E7lJH73Cel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}